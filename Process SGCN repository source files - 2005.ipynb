{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests,io,configparser\n",
    "from IPython.display import display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get API keys and any other config details from a file that is external to the code.\n",
    "config = configparser.RawConfigParser()\n",
    "config.read_file(open(r'../config/stuff.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build base URL with API key using input from the external config.\n",
    "def getBaseURL():\n",
    "    gc2APIKey = config.get('apiKeys','apiKey_GC2_BCB').replace('\"','')\n",
    "    apiBaseURL = \"https://gc2.mapcentia.com/api/v1/sql/bcb?key=\"+gc2APIKey\n",
    "    return apiBaseURL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I was running into encoding errors in the SWAP/SGCN data I originally brought into the GC2 system and I've figure out a bunch of stuff since then, I am building out a process here that will read data from the original files into a new sgcn table in the sgcn schema in our GC2 instance. This will let us re-run the entire thing as necessary from source data in ScienceBase and should set us up for processing new data with some checks on time or existing content.\n",
    "\n",
    "The process below is specific to 2005 SWAP files. First, I found some issues in the existing files and had to clean up a few problems in the sources. 10 of the text files had no first row header. The files were in different original encodings, so I had to put in an exception to find that (text encodings are a pain in the butt!). It seemed like the easiest way to validate the text files, tee them up, and process them into the SGCN table was to read the text files into Pandas dataframes and then iterate over them to put row by row into GC2 via the SQL API. There is probably some better bulk data load method we can figure out eventually, but this lets us validate everything along the way.\n",
    "\n",
    "Putting the data all into a common date table from source files means we can then select unique names from there for processing against taxonomic authorities and retrieving related data. We can then build that information back into the common data table with code.\n",
    "\n",
    "The ScienceBase query is pretty straightforward, but one thing we should do is add the year that a given data file is associated with to make that explicit instead of relying on file name to get the SGCN year. The query can be modified here if we need to process some specific part of the 2005 collection separately again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query ScienceBase for the 2005 states, returning the files structure along with tags (where we get state name)\n",
    "sbQ = \"https://www.sciencebase.gov/catalog/items?q=2005&parentId=56d720ece4b015c306f442d5&format=json&fields=title,files,tags&max=100\"\n",
    "sbR = requests.get(sbQ).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gc2.mapcentia.com/api/v1/sql/bcb?key=1c95cdb240f82acedec84299103e6d4e&q=INSERT INTO sgcn.sgcn                 (sourceid,sgcn_year,sgcn_state,scientificname_submitted,commonname_submitted,taxonomicgroup_submitted)                 VALUES ('https://www.sciencebase.gov/catalog/item/5787cd0ae4b0d27deb3754f2',2005,'Louisiana','Acipenser oxyrhinchus desotoi','Gulf Sturgeon','Fish')\n",
      "https://gc2.mapcentia.com/api/v1/sql/bcb?key=1c95cdb240f82acedec84299103e6d4e&q=INSERT INTO sgcn.sgcn                 (sourceid,sgcn_year,sgcn_state,scientificname_submitted,commonname_submitted,taxonomicgroup_submitted)                 VALUES ('https://www.sciencebase.gov/catalog/item/5787cd0ae4b0d27deb3754f2',2005,'Louisiana','Actinonaias ligamentina','Mucket','Bivalves')\n",
      "https://gc2.mapcentia.com/api/v1/sql/bcb?key=1c95cdb240f82acedec84299103e6d4e&q=INSERT INTO sgcn.sgcn                 (sourceid,sgcn_year,sgcn_state,scientificname_submitted,commonname_submitted,taxonomicgroup_submitted)                 VALUES ('https://www.sciencebase.gov/catalog/item/5787cd0ae4b0d27deb3754f2',2005,'Louisiana','Adinia xenica','Diamond Killifish','Fish')\n",
      "https://gc2.mapcentia.com/api/v1/sql/bcb?key=1c95cdb240f82acedec84299103e6d4e&q=INSERT INTO sgcn.sgcn                 (sourceid,sgcn_year,sgcn_state,scientificname_submitted,commonname_submitted,taxonomicgroup_submitted)                 VALUES ('https://www.sciencebase.gov/catalog/item/5787cd0ae4b0d27deb3754f2',2005,'Louisiana','Aimophila aestivalis','Bachman''s Sparrow','Birds')\n",
      "https://gc2.mapcentia.com/api/v1/sql/bcb?key=1c95cdb240f82acedec84299103e6d4e&q=INSERT INTO sgcn.sgcn                 (sourceid,sgcn_year,sgcn_state,scientificname_submitted,commonname_submitted,taxonomicgroup_submitted)                 VALUES ('https://www.sciencebase.gov/catalog/item/5787cd0ae4b0d27deb3754f2',2005,'Louisiana','Alosa alabamae','Alabama Shad','Fish')\n",
      "Total Records Processed: 5\n"
     ]
    }
   ],
   "source": [
    "totalRecords = 0\n",
    "sgcn_year = 2005\n",
    "\n",
    "for item in sbR['items']:\n",
    "    sgcn_state = item['tags'][0]['name']\n",
    "    sourceid = \"https://www.sciencebase.gov/catalog/item/\"+item['id']\n",
    "    for file in item['files']:\n",
    "        if file['name'][-25:] == 'Species_Original_List.txt':\n",
    "            stateList = requests.get(file['url']).content\n",
    "            try:\n",
    "                stateListPD = pd.read_csv(io.StringIO(stateList.decode('utf-8')))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                stateListPD = pd.read_csv(io.StringIO(stateList.decode('utf-8')), sep='\\t')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                stateListPD = pd.read_csv(io.StringIO(stateList.decode('iso-8859-1')), sep='\\t')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    for ir in stateListPD.itertuples():\n",
    "        if type(ir[1]) is float:\n",
    "            scientificname_submitted = \"\"\n",
    "        else:\n",
    "            scientificname_submitted = ir[1].replace(\"'\",\"''\")\n",
    "        \n",
    "        if scientificname_submitted == \"Scientific Name\":\n",
    "            break\n",
    "        \n",
    "        if type(ir[2]) is float:\n",
    "            commonname_submitted = \"\"\n",
    "        else:\n",
    "            commonname_submitted = ir[2].replace(\"'\",\"''\")\n",
    "\n",
    "        taxonomicgroup_submitted = ir[3]\n",
    "\n",
    "        try:\n",
    "            q = \"INSERT INTO sgcn.sgcn \\\n",
    "                (sourceid,sgcn_year,sgcn_state,scientificname_submitted,commonname_submitted,taxonomicgroup_submitted) \\\n",
    "                VALUES ('\"+sourceid+\"',\"+str(sgcn_year)+\",'\"+sgcn_state+\"','\"+scientificname_submitted+\"','\"+commonname_submitted+\"','\"+taxonomicgroup_submitted+\"')\"\n",
    "            r = requests.get(getBaseURL()+\"&q=\"+q).json()\n",
    "            print (r)\n",
    "            totalRecords = totalRecords+1\n",
    "        except:\n",
    "            display (ir)\n",
    "\n",
    "print (\"Total Records Processed: \"+str(totalRecords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
